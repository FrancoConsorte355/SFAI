# Plan de Monitoreo y An√°lisis de Logging para LibreChat

## 1. Investigaci√≥n de las capacidades de Logging en LibreChat

### Objetivo del Caso de Uso
Implementar un sistema de trazabilidad y an√°lisis que:
- Categoriza autom√°ticamente las preguntas realizadas por los usuarios internos.
- Relacione cada pregunta con el documento o fuente consultada.
- Permite medir qu√© tan √∫til es cada documento y detectar brechas de conocimiento o duplicaciones.

### Modelo Conceptual (a nivel datos)

| Entidad      | Descripci√≥n                                                                 |
|-------------|-----------------------------------------------------------------------------|
| Pregunta     | Consulta realizada por un colaborador (usuario)                            |
| Categor√≠a    | Categor√≠a de la pregunta: Fullstack, Database, Backend, etc.               |
| Rol del usuario | Rol desde el cual se formula la pregunta (puede coincidir o complementar) |
| Documento    | Fuente que el motor utiliza para responder (puede ser doc t√©cnico, README) |
| Log de uso   | Registro detallado del flujo (usuario, hora, √©xito, error, sugerencia usada, etc.) |

## Columnas de la base de datos

- ID de consulta
- Nombre de usuario (en caso de no estar registrado ‚Äî Null)
- Rol de usuario (Full Stack, Frontend, Backend, QA, Data Analyst)
- Texto de la consulta ‚Äî Pregunta realizada
- Documentos consultados
- Tiempo de Respuesta
- Logs ‚Äî Clasificaci√≥n:
  - Emergency
  - Alert
  - Critical
  - Error
  - Warning
  - Notice
  - Informational
  - Debug
- Resultado:
  - √âxito
  - Error
  - Sin respuesta
  - Respuesta parcial
- Feedback del usuario:
  - Like
  - Regenerate
  - Valoraci√≥n manual
- Versi√≥n del modelo/algoritmo ‚Äî Identifica qu√© motor o modelo respondi√≥ la consulta

## 2. M√©tricas para monitorear la salud del servicio

| M√©trica                          | Prop√≥sito                                          | C√°lculo                                                    | Umbral de Alerta / Acci√≥n Recomendada                  |
|----------------------------------|----------------------------------------------------|-------------------------------------------------------------|---------------------------------------------------------|
| Preguntas por categor√≠a          | Detectar qu√© √°reas tienen mayor demanda de conocimiento | Conteo de preguntas agrupadas por categor√≠a                 | > 40% en una categor√≠a ‚Üí revisar documentaci√≥n existente |
| Preguntas por documento consultado | Identificar documentos m√°s consumidos o √∫tiles    | Conteo por documento enlazado por respuesta                | Documento con > 50 consultas/semana = "popular"         |
| Preguntas sin documento asociado | Detectar preguntas no respondidas con contenido actual | Conteo de respuestas sin documentos o sin respuesta         | > 5% por semana ‚Üí indicar necesidad de generar documentaci√≥n |
| Tiempo promedio de respuesta     | Medir eficiencia del sistema de sugerencias        | Diferencia entre hora de pregunta y respuesta              | > 5 segundos = revisar eficiencia del motor              |
| Ratio de re-preguntas (follow-up) | Verificar si las respuestas son claras o incompletas | % de preguntas con preguntas sucesivas del mismo usuario    | > 25% ‚Üí posible brecha de claridad o documentaci√≥n insuficiente |
| Categor√≠a sin cobertura          | Detectar categor√≠as sin respuestas v√°lidas         | Categor√≠as con 0 respuestas exitosas en un periodo         | Revisi√≥n semanal y creaci√≥n de documentos               |
| Usuarios m√°s activos (preguntadores) | Detectar necesidades de onboarding o mentoring     | Agrupaci√≥n por usuario                                     | > 10 preguntas por semana ‚Üí posible acompa√±amiento personalizado |
| √âxito de sugerencia por documento | Evaluar calidad de contenido sugerido              | % de respuestas con feedback positivo / sin re-pregunta por documento | < 70% de √©xito ‚Üí revisar documento                     |

### ¬øC√≥mo instrumentarlo t√©cnicamente?
Cada vez que un colaborador realiza una pregunta, registrar:
- ‚úÖ El texto de la pregunta
- üß© La categor√≠a
- üë§ El rol del usuario
- üìÑ Los documentos consultados para responder
- üïê Tiempos de respuesta
- üì∂ Resultado (respondido, con error, requiere nueva doc)

## 3. Recolecci√≥n de las m√©tricas propuestas

**Herramientas / m√©todos sugeridos:**
- LibreChat como backend generador de actividad.
- MongoDB Atlas como base de datos de almacenamiento.
- Grafana para visualizaci√≥n en tiempo real.

## 4. Plan B√°sico de Monitoreo

**Frecuencia:**
- Tiempo real: Dashboards en Grafana muestran m√©tricas y logs en vivo.
- Revisi√≥n diaria: El equipo de DevOps revisa manualmente dashboards y logs.

**Herramientas:**
- **Grafana**: Visualizaci√≥n en tiempo real.
- **Prometheus**: Recolecci√≥n de m√©tricas.
- **Alertas autom√°ticas**: Notificaciones por Slack, mail o generaci√≥n de tickets en Jira.

**Procedimiento ante anomal√≠as:**
- **Detecci√≥n**: Se activa una alerta cuando alguna m√©trica excede su umbral (ej: latencia > 5s).
- **An√°lisis**: Revisi√≥n de logs recientes para confirmar la causa ra√≠z.
- **Escalamiento**: Notificaci√≥n al equipo t√©cnico para aplicar medidas correctivas.

## 5. Escenario de Prueba de Monitoreo

**Escenario:**
 Simulaci√≥n de respuestas lentas del sistema de sugerencias durante una carga de consultas moderada.
**Contexto:**
 Se detecta que el sistema presenta un tiempo promedio de respuesta de 2.82 segundos, superando el umbral de 2 segundos establecido para medir la eficiencia del motor de sugerencias. Las pruebas se realizan bajo una carga constante de 40 mensajes por segundo (MPS).
 
**Resultado Esperado:**
*Alerta autom√°tica:*
 Se dispara una alerta cuando el promedio de respuesta supera los 2 segundos durante un per√≠odo de m√°s de 1 minuto.


*Logs y dashboards:*
 Muestran la diferencia entre la hora de ingreso de la pregunta y la generaci√≥n de la respuesta, evidenciando la latencia con ejemplos concretos. Se visualiza un patr√≥n consistente de respuestas demoradas.


*Medidas correctivas:*
-Ejecuci√≥n de pruebas de carga:
Simular diferentes niveles de tr√°fico (desde 10 hasta 100 MPS).
Identificar el punto exacto de quiebre en el rendimiento del motor.

-Optimizaci√≥n del motor de sugerencias:
Revisi√≥n de las consultas realizadas al motor: detecci√≥n de preguntas que demandan alto procesamiento (por ejemplo, prompts demasiado largos, anidados o con contextos innecesarios).
Revisi√≥n del modelo: ajuste de hiperpar√°metros, simplificaci√≥n del pipeline de generaci√≥n de respuestas.
Considerar t√©cnicas de truncamiento y resumen para limitar el input que recibe el modelo.

-Implementaci√≥n de cach√© inteligente con Redis:
Cachear respuestas a preguntas frecuentes para reducir tiempo de generaci√≥n.
Uso de hashes del prompt como clave del cach√©.
Establecer pol√≠ticas de expiraci√≥n (TTL) y actualizaci√≥n en background.

-Monitoreo y logging avanzados:
Incluir m√©tricas como p95/p99 del tiempo de respuesta, adem√°s del promedio.
Trazabilidad de cada request-response con logs estructurados.
Dashboards con correlaci√≥n entre carga, latencia y uso de CPU/RAM del motor.

-Balanceo de carga / escalado horizontal:
Aplicar auto-scaling de instancias del motor de sugerencias seg√∫n CPU o latencia.
Uso de colas de mensajes (como Kafka o RabbitMQ) para desacoplar el ingreso de preguntas y su procesamiento.


## 6. Claridad y Precisi√≥n
El documento est√° redactado con lenguaje t√©cnico accesible, estructurado para su implementaci√≥n clara por cualquier miembro del equipo t√©cnico.

## 7. Revisi√≥n Ortogr√°fica
Este documento ha sido revisado y est√° libre de errores ortogr√°ficos y gramaticales.


